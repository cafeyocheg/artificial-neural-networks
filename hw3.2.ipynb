{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28639345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99bae63",
   "metadata": {},
   "source": [
    "Примечание: в качестве дополнительного задания построения и обучения нейросети на новом датасете данные в методичках и ноутбуках разнятся. \n",
    "\n",
    "Где-то дается \"два массива с рукописными буквами и с характеристиками вина.\", или можно вернуться к обычному mnist.\n",
    "\n",
    "А в блокноте с видеоурока - \"Попробуйте обучить нейронную сеть на TensorFlow 2 на датасете imdb_reviews.\"\n",
    "\n",
    "Я решил, в учебных целях, разницы особой нет. Взял последнее.\n",
    "\n",
    "Задача нейросети тривиальна: обучиться на размеченных рецензиях на фильмы на сайте imdb и предсказывать эмоциональную окраску обзоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5429e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36846731",
   "metadata": {},
   "source": [
    "Описание датасета:\n",
    "\n",
    "Датасет IMDb состоит из 50 000 обзоров фильмов от пользователей, помеченных как положительные (1) или отрицательные (0).\n",
    "\n",
    "Рецензии предварительно обрабатываются, каждая из них кодируется последовательностью индексов слов в виде целых чисел.\n",
    "\n",
    "Слова в обзорах индексируются по их общей частоте появления в датасете. Например, целое число «2» кодирует второе наиболее часто используемое слово."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c4993",
   "metadata": {},
   "source": [
    "Загружаем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5977165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddffd224",
   "metadata": {},
   "source": [
    "В нем 50000 рецензий поделены по 25000: для обучения и для тестирования. \n",
    "\n",
    "Поэтому их соединю для последующего разделения 40000/10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d45a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((train_data, test_data), axis=0)\n",
    "\n",
    "labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4dc80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Категорий: [0 1]\n",
      "Уникальных слов: 9998\n"
     ]
    }
   ],
   "source": [
    "print(\"Категорий:\", np.unique(labels))\n",
    "print(\"Уникальных слов:\", len(np.unique(np.hstack(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb1dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В среднем слов на рецензию: 234.8\n",
      "Стандартное отклонение: 172.9\n"
     ]
    }
   ],
   "source": [
    "print(\"В среднем слов на рецензию:\", round(np.mean([len(i) for i in data]), 1))\n",
    "print(\"Стандартное отклонение:\", round(np.std([len(i) for i in data]), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f58bd3",
   "metadata": {},
   "source": [
    "В качестве примера посмотрим на случайную рецензию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c720b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113]\n"
     ]
    }
   ],
   "source": [
    "print(data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f740af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the # and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life\n"
     ]
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[4]] )\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32ea38",
   "metadata": {},
   "source": [
    "Похоже, негативная рецензия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdc062",
   "metadata": {},
   "source": [
    "ПОДГОТОВКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fab97f",
   "metadata": {},
   "source": [
    "Самая большая рецензия содержит максимум 10000 слов. Нужно векторизовать объекты так, чтобы имели одинаковую длину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3954f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    " \n",
    "    \n",
    "data = tf.convert_to_tensor(vectorize(data), np.float32) \n",
    "labels = tf.convert_to_tensor(labels, np.float32) \n",
    "\n",
    "#data = vectorize(data)\n",
    "#labels = np.array(labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45691d3",
   "metadata": {},
   "source": [
    "Деление на тест и трейн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff08694",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = tf.convert_to_tensor(data[:10000])\n",
    "test_y = tf.convert_to_tensor(labels[:10000])\n",
    "train_x = tf.convert_to_tensor(data[10000:])\n",
    "train_y = tf.convert_to_tensor(labels[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da505430",
   "metadata": {},
   "source": [
    "ПОСТРОЕНИЕ МОДЕЛИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32f7427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation = \"elu\", input_shape=(10000, )),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(16, activation = \"elu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4a59ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cc443aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 7s 9ms/step - loss: 0.3480 - accuracy: 0.8551\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2429 - accuracy: 0.9085\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2101 - accuracy: 0.9213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x250331a1eb0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, \n",
    "          batch_size = 64, \n",
    "          epochs=3\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e951d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - loss: 0.2874 - accuracy: 0.8879 - 2s/epoch - 5ms/step\n",
      "\n",
      "Test accuracy: 0.8879\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x,  test_y, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', round(test_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5d042",
   "metadata": {},
   "source": [
    "Итак, нейросеть по содержанию (наличию и частоте встречаемости слов) предсказывает эмоциональную окраску рецензий с точностью $~0.89$\n",
    "\n",
    "Модель склонна переобучаться. Довел разницу между трейном и тестом до минимальных 3.8% на данных параметрах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd7504",
   "metadata": {},
   "source": [
    "P.S. Чтобы было честно:\n",
    "\n",
    "отталкивался от поста на хабре (https://habr.com/ru/post/477630), который подсказал, как читать датасет imdb_reviews и векторизовать данные.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
